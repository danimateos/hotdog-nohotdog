{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the hotdog / no hotdog classifier\n",
    "\n",
    "Started on 13th June 2018. Let's see how far I get.\n",
    "\n",
    "## Project flow:\n",
    "\n",
    "1. Get training data manually: imagenet + google images search for 'hotdog'\n",
    "\n",
    "2. Crop \"manually\" the google images and imagenet to have the same input dimensions\n",
    "\n",
    "3. Train a first example\n",
    "\n",
    "    * Write a writeup for presentations, blog posts, etc.\n",
    "    \n",
    "4. Refine the example: as per _Deep Learning with Python_, page 130. Data augmentation, pretrained network, finetuning (maybe two different blog posts?).\n",
    "\n",
    "    * Write a writeup for presentations, blog posts, etc.\n",
    "\n",
    "5. Automate the training data collection\n",
    "\n",
    "    * Write a writeup for presentations, blog posts, etc.\n",
    "\n",
    "6. Implement data augmentation\n",
    "\n",
    "7. Play around with advanced concepts: Transfer learning, resizing like in fastai, test-time augmentation, stochastic GD with restarts...\n",
    "\n",
    "    * Write a writeup for presentations, blog posts, etc.\n",
    "\n",
    "8. Port the thing to [Android]?\n",
    "\n",
    "[Android]: https://medium.com/joytunes/deploying-a-tensorflow-model-to-android-69d04d1b0cba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting some hotdogs\n",
    "\n",
    "We'll go the easy way: get some results from Google Image Search.\n",
    "\n",
    "https://stackoverflow.com/questions/36438261/extracting-images-from-google-images-using-src-and-beautifulsoup\n",
    "\n",
    "https://gist.github.com/genekogan/ebd77196e4bf0705db51f86431099e57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib3\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Identify ourselves to Google\n",
    "# if we don't pass a header, the response will contain everything but the images\n",
    "header={'User-Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = 'hot+dog'\n",
    "search_type = 'isch'\n",
    "\n",
    "response = requests.get(url='http://www.google.es/search', \n",
    "                        params={'q' : search_term, 'tbm' : search_type},\n",
    "                        headers=header)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go straight for the links, we get #'s. I think this is because the href gets populated dinamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_end = soup.findAll('a', {'class' : 'rg_l'})\n",
    "[a['href'] for a in dead_end[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did the guys in \n",
    "# https://gist.github.com/genekogan/ebd77196e4bf0705db51f86431099e57\n",
    "# find this?\n",
    "\n",
    "divs = soup.find_all(\"div\",{\"class\":\"rg_meta\"})\n",
    "props = [json.loads(div.text) for div in divs]\n",
    "image_sources = [(prop['ou'], prop['ity']) for prop in props]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sources[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "target_folder = 'data/hotdogsfromgoogle'\n",
    "os.makedirs(target_folder, exist_ok=True)\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "for image_url, image_type in image_sources:\n",
    "    try:\n",
    "        response = http.request('GET', image_url)\n",
    "        if response.status == 200:\n",
    "            filename = image_url.split('/')[-1].split('?')[0]\n",
    "            handle = open(os.path.join(target_folder, filename), 'wb')\n",
    "            handle.write(response.data)\n",
    "        else: \n",
    "            print(f'Something went wrong with image {image_url}')\n",
    "            \n",
    "    finally:\n",
    "        handle.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more than 100: we will need to find the link to the next page, I guess\n",
    "\n",
    "# For that, we need selenium\n",
    "# Part 2 of the blog entry?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of blog posts to write\n",
    "\n",
    "In no particular order\n",
    "\n",
    "* Training data collection: ImageNet\n",
    "\n",
    "* Splitting into train, validation and test set\n",
    "\n",
    "* First approximation, overfitting,  data augmentation\n",
    "\n",
    "* Dropout to combat overfitting.\n",
    "\n",
    "* Transfer learning: Using a pretrained network, then training the last layer, then unfreezing and training all layers with a small learning rate.\n",
    "\n",
    "* Confusion matrix? where?\n",
    "\n",
    "* Cross validation?\n",
    "\n",
    "* Prototyping your model locally with a small subset of images then training it in the cloud (?). Paperspace?\n",
    "\n",
    "* Porting a learned model to mobile (?), and how to minimize disk memory and usage by quantizing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "[diff_learning_rates_1]: https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "\n",
    "[diff_learning_rates_2]: https://github.com/keras-team/keras/issues/898\n",
    "\n",
    "[Android]: https://medium.com/joytunes/deploying-a-tensorflow-model-to-android-69d04d1b0cba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doubts\n",
    "\n",
    "Why is normalization of input variable values positive or required for Deep Learning?\n",
    "\n",
    "Where does it make more sense to put a dropout layer?? After a conv, before..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
