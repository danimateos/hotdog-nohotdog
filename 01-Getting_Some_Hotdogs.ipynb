{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet\n",
    "\n",
    "We need a set of images that are NOT hotdogs in order to be able to train our algorithm. For that we can use [ImageNet]. \n",
    "\n",
    "ImageNet is a set of tagged images that has been used for a yearly competition whose 2012 edition is credited with starting the current Deep Learning boom.\n",
    "\n",
    "![The contest that started it](https://blogs.nvidia.com/wp-content/uploads/2016/06/DefenseAIPicture3-002.png)\n",
    "\n",
    "The thing is, ImageNet contains more than a million images totalling around 100GB. Not only would that take a long time to download: it would be prohibitively costly to train. We are going to download only a part of it for now.\n",
    "\n",
    "If I give up I can always use [this](https://github.com/xkumiyu/imagenet-downloader/)\n",
    "\n",
    "[ImageNet]: http://image-net.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that, we need to understand how ImageNet is structured. It is based upon [WordNet], a lexical database of English. WordNet contains nouns, verbs, adjectives and adverbs grouped into _synsets_ (sets of synonyms), but ImageNet contains images corresponding only to the nouns.\n",
    "\n",
    "It actually has a hotdog term, n07697537.\n",
    "\n",
    "Each synset is identified by its wnid (WordNet id).\n",
    "\n",
    "ImageNet doesn't own the images, so they only provide them after a registration and a request promising to use them for non-commercial research and/or educational use. \n",
    "\n",
    "However, they do provide the image urls freely, so we are going to use our downloader to get them. We will need to get the urls first.\n",
    "\n",
    "[WordNet]: https://wordnet.princeton.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# TODO: async retrieve images\n",
    "\n",
    "synset_ids = 'http://image-net.org/archive/words.txt'\n",
    "\n",
    "filename = synset_ids.split('/')[-1]\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    \n",
    "    response = requests.get(synset_ids, headers=header)\n",
    "    f = open(filename, 'wb')\n",
    "    f.write(response.content)\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "with open(filename) as f:\n",
    "    \n",
    "    wnids = {line.split()[0]: line.split()[1] for line in f.readlines()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wnids are overlapping, that is, there are general terms and more specific terms that are contained within them (hyponyms). The easiest way to deal with this is to just get the urls for the images we want and deduplicate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also have to deal with the hotdogs in the set. That should be easy though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def urls_from_wnid(wnid):\n",
    "    response = requests.get('http://www.image-net.org/api/text/imagenet.synset.geturls', params={'wnid' : wnid})\n",
    "    urls = response.content.decode('utf-8').splitlines()\n",
    "    \n",
    "    return urls\n",
    "\n",
    "urls_from_wnid('n07697537')     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would work, but it is extremely slow because we are making over 80000 requests.\n",
    "\n",
    "Also, the damn imagenet site is dog slow. I let it running overnight so you see just how slow it can be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# all_urls = {url for wnid in wnids.keys() for url in urls_from_wnid(wnid)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good nested list comprehension explanation: https://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hotdog_wnids = {wnid for wnid, term in wnids.items() if 'hot' in term and 'dog' in term}\n",
    "hotdog_urls = {url for wnid in hotdog_wnids for url in urls_from_wnid(wnid)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hotdog_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading all image urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "imagenet_fall11_urls = 'http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz'\n",
    "filename = imagenet_fall11_urls.split('/')[-1]\n",
    "\n",
    "# Don't want to redownload if already have it\n",
    "if not os.path.exists(filename):\n",
    "    response = request.get(imagenet_fall11_urls, headers=header)\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import tarfile\n",
    "\n",
    "tf = tarfile.open('imagenet_fall11_urls.tgz')\n",
    "\n",
    "# Shortcut: I know it only contains one file\n",
    "content = tf.extractfile(tf.getmembers()[0])\n",
    "\n",
    "wnid_urls = []\n",
    "for line in content:\n",
    "    try:\n",
    "        wnid, url = line.decode('utf-8')[:-1].split('\\t')\n",
    "    except:\n",
    "        print(line)\n",
    "        \n",
    "    wnid_urls.append((wnid, url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wnid_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = {url for wnid, url in wnid_urls}\n",
    "# We already have the hotdog urls from before> tidy up \n",
    "#hotdog_urls = {url for wnid, url in wnid_urls if wnid in hotdog_wnids}\n",
    "other_urls = all_urls - hotdog_urls\n",
    "\n",
    "print((len(all_urls), len(hotdog_urls), len(other_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "hotdogs_sample = random.sample(hotdog_urls, 1216) # Use all for now\n",
    "nohotdogs_sample = random.sample(other_urls, k=5000)\n",
    "\n",
    "hd_train, hd_val = train_test_split(hotdogs_sample)\n",
    "nohd_train, nohd_val = train_test_split(nohotdogs_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "debug=False\n",
    "\n",
    "folders_urls = { os.path.join(train_dir, 'hotdog'): hd_train,\n",
    "                 os.path.join(train_dir, 'nohotdog'): nohd_train,\n",
    "                 os.path.join(validation_dir, 'hotdog'): hd_val,\n",
    "                 os.path.join(validation_dir, 'nohotdog'): hd_val }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/24398044/downloading-a-lot-of-files-using-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should actually use a logging specific module, but let's keep things simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import imghdr\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "log = open('log_img_download', 'w')\n",
    "\n",
    "for folder, urls in folders_urls.items():\n",
    "    ok = 0\n",
    "    fail = 0\n",
    "    shutil.rmtree(folder, ignore_errors=True)\n",
    "    os.makedirs(folder)\n",
    "    \n",
    "    log.write(f'{datetime.now()} Starting with {folder}\\n')\n",
    "    \n",
    "    for image_url in urls:\n",
    "        try:\n",
    "            response = requests.get(image_url)\n",
    "            \n",
    "            if response.status_code == 200 and imghdr.what(None, h=response.content) in ['jpeg', 'png']:\n",
    "                filename = image_url.split('/')[-1].split('?')[0]\n",
    "                \n",
    "                f = open(os.path.join(folder, filename), 'wb')\n",
    "                f.write(response.content)\n",
    "                n += 1 \n",
    "                if debug: log.write(f'{datetime.now()} wrote {filename}\\n')\n",
    "                ok += 0\n",
    "            else: \n",
    "                log.write(f'{datetime.now()} {response.status_code} response for {image_url} of type {imghdr.what(None, h=response.content)}\\n')\n",
    "                fail += 0\n",
    "                \n",
    "        except Exception as e:\n",
    "            log.write(f'{datetime.now()} Something went wrong with image {image_url}: {e}\\n')\n",
    "                    \n",
    "    log.write(f'{datetime.now()} Wrote {ok} images in folder {folder}, with {fail} failures\\n')\n",
    "\n",
    "log.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: either get all the images and subset, or keep retrying connections until I get the desired number of images. There are _a lot_ of broken links.\n",
    "\n",
    "https://stackoverflow.com/questions/36554365/downloading-many-images-with-python-requests-and-multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: simultaneous requests for speedup\n",
    "\n",
    "Tidy up\n",
    "\n",
    "Convert into proper blog post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://image-net.org/synset?wnid=n07697537\n",
    "\n",
    "http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07697537"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://caffe.berkeleyvision.org/gathered/examples/imagenet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "\n",
    "Last layer -> sigmoid (1 vs all classification)\n",
    "\n",
    "How to calculate learning rates with Keras?? -> maybe another blog post\n",
    "\n",
    "  * Somewhat related: what does Jeremy Howard refer to as each of the three parts of a net? and can you set differential learning rates in Keras??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
